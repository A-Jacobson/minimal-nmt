{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import hyperparams as hp\n",
    "from datasets import load_dataset\n",
    "from models import Encoder, Decoder, Seq2Seq\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "train_iter, val_iter, test_iter, DE, EN = load_dataset(batch_size=1, device=-1)\n",
    "\n",
    "encoder = Encoder(source_vocab_size=len(DE.vocab),\n",
    "                  embed_dim=hp.embed_dim, hidden_dim=hp.hidden_dim,\n",
    "                  n_layers=hp.n_layers, dropout=hp.dropout)\n",
    "decoder = Decoder(target_vocab_size=len(EN.vocab),\n",
    "                  embed_dim=hp.embed_dim, hidden_dim=hp.hidden_dim,\n",
    "                  n_layers=hp.n_layers, dropout=hp.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "start_token = batch.trg[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embed): Embedding(10839, 256, padding_idx=1)\n",
       "  (attention): LuongAttention(\n",
       "    (W): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (gru): GRU(768, 512, num_layers=2, dropout=0.2)\n",
       "  (out): Linear(in_features=1024, out_features=10839, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out, encoder_hidden = encoder(batch.src)\n",
    "decoder_hidden = encoder_hidden[-decoder.n_layers:]  # take what we need from encoder\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam:\n",
    "    def __init__(self, beam_width):\n",
    "        self.heap = list()\n",
    "        self.beam_width = beam_width\n",
    "\n",
    "    def add(self, score, sequence, hidden_state):\n",
    "        \"\"\"\n",
    "        maintains a heap of size(beam_width), always removes lowest scoring nodes.\n",
    "        \"\"\"\n",
    "        heapq.heappush(self.heap, (score, sequence, hidden_state))\n",
    "        if len(self.heap) > self.beam_width:\n",
    "            heapq.heappop(self.heap)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.heap)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.heap)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.heap[idx]\n",
    "\n",
    "\n",
    "def beamsearch(decoder_topk, beam_size=2, maxlen=20):\n",
    "    beam = Beam(beam_size)  # starting layer in search tree\n",
    "    beam.add(1.0, batch.trg[0:1], decoder_hidden)  # initialize root\n",
    "    for _ in range(maxlen):\n",
    "        # expand next layer up to maxlen times\n",
    "        next_beam = Beam(beam_size)\n",
    "        # Add complete sentences that do not yet have the best probability to the current beam, the rest prepare to add more words to them.\n",
    "        # for node in previous layer\n",
    "        for node in beam:  # each layer will only have (beam_width) nodes\n",
    "            # Get probability of each possible next word for the incomplete prefix.\n",
    "            score, sequence, hidden_state = node\n",
    "            next_probs, next_words, hidden_state = get_next(sequence[-1:],\n",
    "                                                            hidden_state,\n",
    "                                                            beam_size)\n",
    "            for i in range(beam_size):\n",
    "                score = score * next_probs[i]\n",
    "                # add next word to sequence\n",
    "                sequence = torch.cat([sequence, next_words[i]])\n",
    "                next_beam.add(score, sequence, hidden_state)\n",
    "\n",
    "        # move down one layer (to the next word in sequence up to maxlen)\n",
    "        beam = next_beam\n",
    "    best_score, best_sequence, _ = max(beam)  # get highest scoring sequence\n",
    "    return best_score, best_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(last_word, hidden_state, k=3):\n",
    "    \"\"\"\n",
    "    Given the last item in a sequence and the hidden state used to generate the sequence\n",
    "    return the top3 most likely words and their scores\n",
    "    \"\"\"\n",
    "    output, hidden_state, _ = decoder(last_word, encoder_out, hidden_state)\n",
    "    nex_word_probs = F.softmax(output, dim=2)\n",
    "    probabilites, next_words = nex_word_probs.topk(k)\n",
    "    return probabilites.squeeze().data, next_words.view(k, 1, 1), hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score, best_seq = beamsearch(decoder_topk, beam_size=3)\n",
    "best_score, best_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a decoding helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beamsearch import BeamHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(encoder, decoder)\n",
    "beam_helper = BeamHelper(beam_size=3, maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 s, sys: 0 ns, total: 1.3 s\n",
      "Wall time: 327 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.087069415258347e-79, Variable containing:\n",
       "      2\n",
       "    662\n",
       "   6344\n",
       "   2850\n",
       "   4007\n",
       "   2850\n",
       "   6170\n",
       "   6170\n",
       "   6170\n",
       "   7784\n",
       "   8470\n",
       "   1751\n",
       "   1751\n",
       "   3824\n",
       "    487\n",
       "  10693\n",
       "    959\n",
       "   2850\n",
       "   2850\n",
       "   3897\n",
       "   3897\n",
       " [torch.LongTensor of size 21x1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time seq2seq(batch.src, beam_helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increased beam size comes at a cost\n",
    "with beam_size == output vocab beam search becomes breath first search. with beam_size == 1 we get greedy search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.97 s, sys: 10.6 ms, total: 1.98 s\n",
      "Wall time: 499 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.087069415258347e-79, Variable containing:\n",
       "      2\n",
       "    662\n",
       "   6344\n",
       "   2850\n",
       "   4007\n",
       "   2850\n",
       "   6170\n",
       "   6170\n",
       "   6170\n",
       "   7784\n",
       "   8470\n",
       "   1751\n",
       "   1751\n",
       "   3824\n",
       "    487\n",
       "  10693\n",
       "    959\n",
       "   2850\n",
       "   2850\n",
       "   3897\n",
       "   3897\n",
       " [torch.LongTensor of size 21x1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_helper = BeamHelper(beam_size=5, maxlen=20)\n",
    "%time seq2seq(batch.src, beam_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.58 s, sys: 51.2 ms, total: 7.63 s\n",
      "Wall time: 1.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.087069415258347e-79, Variable containing:\n",
       "      2\n",
       "    662\n",
       "   6344\n",
       "   2850\n",
       "   4007\n",
       "   2850\n",
       "   6170\n",
       "   6170\n",
       "   6170\n",
       "   7784\n",
       "   8470\n",
       "   1751\n",
       "   1751\n",
       "   3824\n",
       "    487\n",
       "  10693\n",
       "    959\n",
       "   2850\n",
       "   2850\n",
       "   3897\n",
       "   3897\n",
       " [torch.LongTensor of size 21x1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_helper = BeamHelper(beam_size=20, maxlen=20)\n",
    "%time seq2seq(batch.src, beam_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
